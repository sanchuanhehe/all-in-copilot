# All-In Copilot

ğŸš€ SDK & CLI for building VS Code Chat extensions with custom LLM providers.

## âœ¨ Features

- **CLI ä¸€é”®ç”Ÿæˆ**: äº¤äº’å¼åˆ›å»ºæ‰©å±•é¡¹ç›®
- **å¤šä¾›åº”å•†é¢„è®¾**: å†…ç½® GLMã€DeepSeekã€Qwenã€MiniMaxã€OpenAIã€Anthropic é…ç½®
- **åŠ¨æ€æ¨¡å‹è·å–**: è‡ªåŠ¨ä» API è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- **å³ç”¨å‹æ¨¡æ¿**: å¤åˆ¶æ¨¡æ¿å³å¯å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰ Copilot æ‰©å±•
- **è½»é‡ SDK**: æ ¸å¿ƒ SDK æ—  VS Code ä¾èµ–ï¼Œå¯åœ¨ä»»ä½• Node.js ç¯å¢ƒä½¿ç”¨

## Architecture

```
all-in-copilot/
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ sdk/              # Core SDK
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ core/     # Types, model fetcher
â”‚           â””â”€â”€ vscode/   # VS Code provider helpers
â”‚
â”œâ”€â”€ templates/            # Extension templates
â”‚   â”œâ”€â”€ base-template/    # Base template for custom providers
â”‚   â”œâ”€â”€ glm-template/     # GLM (æ™ºè°±AI) example
â”‚   â””â”€â”€ minimax-template/ # MiniMax example
â”‚
â””â”€â”€ cli/                  # Project generator CLI
    â””â”€â”€ src/
        â””â”€â”€ index.ts
```

## Quick Start

### Method 1: Use CLI (Recommended)

```bash
# Install CLI globally
npm install -g @all-in-copilot/cli

# Create a new project interactively
all-in-copilot

# Or with short command
aic create my-copilot
```

### Method 2: Copy Template

```bash
# Copy template
cp -r templates/glm-template my-copilot
cd my-copilot

# Edit configuration
vim src/config.ts

# Install dependencies
npm install

# Compile and test (F5 in VS Code)
npm run compile
```

## CLI Commands

```bash
all-in-copilot              # Interactive mode
all-in-copilot create NAME  # Create project with prompts
all-in-copilot list         # List available presets
all-in-copilot help         # Show help
```

### Available Presets

| Preset    | Provider       | API Format |
|-----------|----------------|------------|
| glm       | GLM (æ™ºè°±AI)   | OpenAI     |
| minimax   | MiniMax        | Anthropic  |
| kimi      | Kimi (Moonshot)| Anthropic  |
| mimo      | Xiaomi MiMo    | Anthropic  |
| deepseek  | DeepSeek       | OpenAI     |
| qwen      | Qwen (é€šä¹‰åƒé—®) | OpenAI     |
| openai    | OpenAI         | OpenAI     |
| anthropic | Anthropic      | Anthropic  |
| custom    | Custom         | OpenAI     |

## Provider Configuration

Edit `src/config.ts` to customize your provider:

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
  id: 'provider-id',
  name: 'Provider Name',
  baseUrl: 'https://api.example.com/v1/chat/completions',
  apiKeySecret: 'extension-name.apiKey',
  family: 'provider-family',
  apiMode: 'openai',  // 'openai' | 'anthropic' | 'gemini' | 'ollama'
  supportsTools: true,
  supportsVision: false,
  defaultMaxOutputTokens: 4096,
  defaultContextLength: 32768,
  dynamicModels: true,
  modelsCacheTTL: 5 * 60 * 1000,
};

export const FALLBACK_MODELS: ModelConfig[] = [
  {
    id: 'model-1',
    name: 'Model 1',
    maxInputTokens: 30000,
    maxOutputTokens: 4096,
    supportsTools: true,
    supportsVision: false,
  },
];

// Optional: Filter which models to display
export function filterModels(models: ModelConfig[]): ModelConfig[] {
  return models.filter(m => m.id.includes('chat'));
}
```

## SDK Usage

```typescript
import {
  convertToOpenAI,
  convertToolsToOpenAI,
  processOpenAIStream,
  fetchModelsFromAPI,
  estimateTokens,
} from '@all-in-copilot/sdk';
```

// Dynamic model fetching
const providerConfig: ProviderConfig = {
  id: 'my-provider',
  name: 'My Provider',
  baseUrl: 'https://api.example.com/v1',
  // ... other config
};

const models = await fetchModels(providerConfig, { apiKey: 'your-api-key' });
console.log('Available models:', models);

// Use OpenAI-compatible provider
const provider = new OpenAIProvider(providerConfig, { apiKey: 'your-api-key' });
for (const model of models) {
  provider.registerModel(model);
}

const response = await provider.complete({
  model: models[0].id,
  messages: [{ role: 'user', content: 'Hello!' }],
});
```

## Development

### Build SDK

```bash
cd packages/sdk
npm run build
```

### Build VSCode Extension

```bash
cd packages/vscode
npm run compile
```

### Watch Mode

```bash
# Terminal 1
cd packages/sdk && npm run watch

# Terminal 2
cd packages/vscode && npm run watch
```

## CI/CD & Publishing

### GitHub Secrets Required

åœ¨ä»“åº“è®¾ç½®ä¸­æ·»åŠ ä»¥ä¸‹ Secretsï¼š

| Secret | Description |
|--------|-------------|
| `VSCE_PAT` | VS Code Marketplace Personal Access Token |
| `OVSX_PAT` | Open VSX Registry Token (å¯é€‰) |

### Workflows

| Workflow | Trigger | Description |
|----------|---------|-------------|
| **CI** | Push/PR to `main` | æ„å»º SDKã€CLI å’Œæ‰€æœ‰æ¨¡æ¿ï¼Œè¿è¡Œæµ‹è¯• |
| **Release** | Tag push (`v*`) / Manual | å‘å¸ƒæ­£å¼ç‰ˆæˆ–é¢„å‘å¸ƒç‰ˆåˆ°æ’ä»¶å¸‚åœº |
| **Pre-release** | Push to `pre-release/*` / Manual | å‘å¸ƒé¢„å‘å¸ƒç‰ˆæœ¬ |
| **Publish Single** | Manual only | æ‰‹åŠ¨å‘å¸ƒå•ä¸ªæ’ä»¶ |

### Release Process

#### æ­£å¼å‘å¸ƒ (Stable Release)

```bash
# 1. æ›´æ–°ç‰ˆæœ¬å·
cd templates/glm-template
npm version patch  # or minor / major

# 2. æäº¤å¹¶åˆ›å»º tag
git add .
git commit -m "chore: bump glm-template to v0.2.0"
git tag glm-template-v0.2.0
git push origin main --tags
```

#### é¢„å‘å¸ƒ (Pre-release)

**æ–¹æ³• 1: ä½¿ç”¨ Tag**
```bash
git tag glm-template-v0.2.0-beta.1
git push origin --tags
```

**æ–¹æ³• 2: ä½¿ç”¨åˆ†æ”¯**
```bash
git checkout -b pre-release/glm-template
git push origin pre-release/glm-template
```

**æ–¹æ³• 3: æ‰‹åŠ¨è§¦å‘**
1. æ‰“å¼€ GitHub Actions
2. é€‰æ‹© "Pre-release" workflow
3. ç‚¹å‡» "Run workflow"
4. é€‰æ‹©æ¨¡æ¿å’Œé€‰é¡¹

#### å…¨éƒ¨å‘å¸ƒ

```bash
# å‘å¸ƒæ‰€æœ‰æ¨¡æ¿çš„æ­£å¼ç‰ˆ
git tag v1.0.0
git push origin --tags

# æˆ–é€šè¿‡ workflow_dispatch æ‰‹åŠ¨è§¦å‘
```

### Tag å‘½åè§„èŒƒ

| Pattern | è§¦å‘ | ç¤ºä¾‹ |
|---------|------|------|
| `v*` | å‘å¸ƒæ‰€æœ‰æ¨¡æ¿ | `v1.0.0` |
| `<template>-v*` | å‘å¸ƒæŒ‡å®šæ¨¡æ¿ | `glm-template-v0.2.0` |
| `*-pre*`, `*-alpha*`, `*-beta*`, `*-rc*` | é¢„å‘å¸ƒ | `v1.0.0-beta.1` |

## Templates

### MiniMax Template
- **Base URL**: `https://api.minimaxi.com/anthropic/v1/messages`
- **API Mode**: Anthropic (ä½¿ç”¨ Anthropic å…¼å®¹æ¥å£)
- **Dynamic Models**: âŒ Disabled (ä½¿ç”¨é¢„å®šä¹‰æ¨¡å‹åˆ—è¡¨)
- **Fallback Models**: MiniMax-M2.1, MiniMax-M2.1-lightning, MiniMax-M2

### Kimi Template (Moonshot)
- **Base URL**: `https://api.moonshot.cn/anthropic`
- **API Mode**: Anthropic (Kimi K2 ç³»åˆ—)
- **Dynamic Models**: âŒ Disabled
- **Models**: kimi-k2-thinking-turbo, kimi-k2-thinking, kimi-k2-turbo-preview, kimi-k2-0905-preview

### MiMo Template (Xiaomi)
- **Base URL**: `https://api.xiaomimimo.com/anthropic/v1/messages`
- **API Mode**: Anthropic
- **Dynamic Models**: âŒ Disabled
- **Models**: mimo-v2-flash

### GLM Template (æ™ºè°±AI)
- **Base URL**: `https://open.bigmodel.cn/api/paas/v4/chat/completions`
- **API Mode**: OpenAI (ä¹Ÿæ”¯æŒ Anthropic æ¨¡å¼ç”¨äº GLM Coding Plan)
- **Anthropic URL**: `https://open.bigmodel.cn/api/anthropic`
- **Dynamic Models**: âœ… Enabled (OpenAI æ¨¡å¼)
- **Fallback Models**: GLM-4 Plus, GLM-4, GLM-4V, GLM-3 Turbo

### Base Template
- **Purpose**: Blank template for custom providers
- **API Mode**: Supports both OpenAI and Anthropic formats
- **Dynamic Models**: âœ… Enabled (set `dynamicModels: true` in config)
- **Edit**: `src/config.ts` to configure your provider

## How Dynamic Model Fetching Works

1. Extension calls `/models` endpoint on provider's API
2. Response is parsed and converted to `ModelConfig[]`
3. Models are cached for `modelsCacheTTL` milliseconds
4. If fetch fails, fallback to `FALLBACK_MODELS`
5. Optional `filterModels()` function filters displayed models

```
Provider API (/models)
       â†“
   fetch + cache
       â†“
 filterModels()
       â†“
VS Code Model List
```

## License

MIT
