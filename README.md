# All-In Copilot

Multi-API LLM VSCode extension framework with SDK and CLI build tools.

## âœ¨ Features

- **åŠ¨æ€æ¨¡åž‹èŽ·å–**: è‡ªåŠ¨ä»Ž LLM æä¾›å•† API èŽ·å–å¯ç”¨æ¨¡åž‹åˆ—è¡¨
- **å¤šæä¾›å•†æ”¯æŒ**: å†…ç½® MiniMaxã€GLMï¼ˆæ™ºè°±ï¼‰ã€OpenAIã€Anthropic æ”¯æŒ
- **å³ç”¨åž‹æ¨¡æ¿**: å¤åˆ¶æ¨¡æ¿å³å¯å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰ Copilot æ‰©å±•
- **SDK åˆ†ç¦»**: æ ¸å¿ƒ SDK æ—  VSCode ä¾èµ–ï¼Œå¯åœ¨ä»»ä½• Node.js çŽ¯å¢ƒä½¿ç”¨

## Architecture

```
all-in-copilot/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ sdk/              # Core SDK (no VSCode dependencies)
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ core/     # Types, model fetcher
â”‚   â”‚       â”œâ”€â”€ providers/ # OpenAI, Anthropic providers
â”‚   â”‚       â””â”€â”€ utils/    # Token counter, message converter
â”‚   â”‚
â”‚   â””â”€â”€ vscode/           # VSCode plugin wrapper
â”‚       â””â”€â”€ src/
â”‚
â”œâ”€â”€ templates/            # Extension templates (dynamic models)
â”‚   â”œâ”€â”€ minimax-template/ # MiniMax API
â”‚   â”œâ”€â”€ glm-template/     # GLM (æ™ºè°±AI) API
â”‚   â””â”€â”€ base-template/    # Custom provider base
â”‚
â””â”€â”€ cli/                  # Build CLI tool
    â””â”€â”€ build.ts          # One-click extension builder
```

## Quick Start

### 1. Use Pre-built Templates

Copy a template and modify `src/config.ts`:

```bash
# Copy template
cp -r templates/minimax-template my-copilot
cd my-copilot

# Edit configuration
vim src/config.ts

# Install dependencies
npm install

# Compile
npm run compile

# Test in VSCode (F5)
```

### 2. Provider Configuration

Edit `src/config.ts` to customize your provider:

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
  id: 'provider-id',
  name: 'Provider Name',
  baseUrl: 'https://api.example.com/v1/chat/completions',
  apiKeySecret: 'extension-name.apiKey',
  family: 'provider-family',
  supportsTools: true,
  supportsVision: false,
  defaultMaxOutputTokens: 4096,
  defaultContextLength: 32768,
  // ðŸ†• Enable dynamic model fetching
  dynamicModels: true,
  modelsCacheTTL: 5 * 60 * 1000, // Cache for 5 minutes
};

// Fallback models (used when dynamic fetch fails)
export const FALLBACK_MODELS: ModelConfig[] = [
  {
    id: 'model-1',
    name: 'Model 1',
    maxInputTokens: 30000,
    maxOutputTokens: 4096,
    supportsTools: true,
    supportsVision: false,
  },
];

// Optional: Filter which models to display
export function filterModels(models: ModelConfig[]): ModelConfig[] {
  return models.filter(m => m.id.includes('chat'));
}
```

### 3. Use SDK Directly

```typescript
import { OpenAIProvider, fetchModels, type ProviderConfig } from '@all-in-copilot/sdk';

// Dynamic model fetching
const providerConfig: ProviderConfig = {
  id: 'my-provider',
  name: 'My Provider',
  baseUrl: 'https://api.example.com/v1',
  // ... other config
};

const models = await fetchModels(providerConfig, { apiKey: 'your-api-key' });
console.log('Available models:', models);

// Use OpenAI-compatible provider
const provider = new OpenAIProvider(providerConfig, { apiKey: 'your-api-key' });
for (const model of models) {
  provider.registerModel(model);
}

const response = await provider.complete({
  model: models[0].id,
  messages: [{ role: 'user', content: 'Hello!' }],
});
```

## Development

### Build SDK

```bash
cd packages/sdk
npm run build
```

### Build VSCode Extension

```bash
cd packages/vscode
npm run compile
```

### Watch Mode

```bash
# Terminal 1
cd packages/sdk && npm run watch

# Terminal 2
cd packages/vscode && npm run watch
```

## Templates

### MiniMax Template
- **Base URL**: `https://api.minimax.chat/v1/text/chatcompletion_v2`
- **Dynamic Models**: âœ… Enabled
- **Fallback Models**: abab6.5s-chat, abab6.5-chat, abab5.5-chat

### GLM Template (æ™ºè°±AI)
- **Base URL**: `https://open.bigmodel.cn/api/paas/v4/chat/completions`
- **Dynamic Models**: âœ… Enabled
- **Fallback Models**: GLM-4 Plus, GLM-4, GLM-4V, GLM-3 Turbo

### Base Template
- **Purpose**: Blank template for custom providers
- **Dynamic Models**: âœ… Enabled (set `dynamicModels: true` in config)
- **Edit**: `src/config.ts` to configure your provider

## How Dynamic Model Fetching Works

1. Extension calls `/models` endpoint on provider's API
2. Response is parsed and converted to `ModelConfig[]`
3. Models are cached for `modelsCacheTTL` milliseconds
4. If fetch fails, fallback to `FALLBACK_MODELS`
5. Optional `filterModels()` function filters displayed models

```
Provider API (/models)
       â†“
   fetch + cache
       â†“
 filterModels()
       â†“
VS Code Model List
```

## License

MIT
