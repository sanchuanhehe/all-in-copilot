# All-In Copilot

ðŸš€ SDK & CLI for building VS Code Chat extensions with custom LLM providers.

## âœ¨ Features

- **CLI ä¸€é”®ç”Ÿæˆ**: äº¤äº’å¼åˆ›å»ºæ‰©å±•é¡¹ç›®
- **å¤šä¾›åº”å•†é¢„è®¾**: å†…ç½® GLMã€DeepSeekã€Qwenã€MiniMaxã€OpenAIã€Anthropic é…ç½®
- **åŠ¨æ€æ¨¡åž‹èŽ·å–**: è‡ªåŠ¨ä»Ž API èŽ·å–å¯ç”¨æ¨¡åž‹åˆ—è¡¨
- **å³ç”¨åž‹æ¨¡æ¿**: å¤åˆ¶æ¨¡æ¿å³å¯å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰ Copilot æ‰©å±•
- **è½»é‡ SDK**: æ ¸å¿ƒ SDK æ—  VS Code ä¾èµ–ï¼Œå¯åœ¨ä»»ä½• Node.js çŽ¯å¢ƒä½¿ç”¨

## Architecture

```
all-in-copilot/
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ sdk/              # Core SDK
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ core/     # Types, model fetcher
â”‚           â””â”€â”€ vscode/   # VS Code provider helpers
â”‚
â”œâ”€â”€ templates/            # Extension templates
â”‚   â”œâ”€â”€ base-template/    # Base template for custom providers
â”‚   â”œâ”€â”€ glm-template/     # GLM (æ™ºè°±AI) example
â”‚   â””â”€â”€ minimax-template/ # MiniMax example
â”‚
â””â”€â”€ cli/                  # Project generator CLI
    â””â”€â”€ src/
        â””â”€â”€ index.ts
```

## Quick Start

### Method 1: Use CLI (Recommended)

```bash
# Install CLI globally
npm install -g @all-in-copilot/cli

# Create a new project interactively
all-in-copilot

# Or with short command
aic create my-copilot
```

### Method 2: Copy Template

```bash
# Copy template
cp -r templates/glm-template my-copilot
cd my-copilot

# Edit configuration
vim src/config.ts

# Install dependencies
npm install

# Compile and test (F5 in VS Code)
npm run compile
```

## CLI Commands

```bash
all-in-copilot              # Interactive mode
all-in-copilot create NAME  # Create project with prompts
all-in-copilot list         # List available presets
all-in-copilot help         # Show help
```

### Available Presets

| Preset    | Provider       | API Format |
|-----------|----------------|------------|
| glm       | GLM (æ™ºè°±AI)   | OpenAI     |
| minimax   | MiniMax        | OpenAI     |
| deepseek  | DeepSeek       | OpenAI     |
| qwen      | Qwen (é€šä¹‰åƒé—®) | OpenAI     |
| openai    | OpenAI         | OpenAI     |
| anthropic | Anthropic      | Anthropic  |
| custom    | Custom         | OpenAI     |

## Provider Configuration

Edit `src/config.ts` to customize your provider:

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
  id: 'provider-id',
  name: 'Provider Name',
  baseUrl: 'https://api.example.com/v1/chat/completions',
  apiKeySecret: 'extension-name.apiKey',
  family: 'provider-family',
  apiMode: 'openai',  // 'openai' | 'anthropic' | 'gemini' | 'ollama'
  supportsTools: true,
  supportsVision: false,
  defaultMaxOutputTokens: 4096,
  defaultContextLength: 32768,
  dynamicModels: true,
  modelsCacheTTL: 5 * 60 * 1000,
};

export const FALLBACK_MODELS: ModelConfig[] = [
  {
    id: 'model-1',
    name: 'Model 1',
    maxInputTokens: 30000,
    maxOutputTokens: 4096,
    supportsTools: true,
    supportsVision: false,
  },
];

// Optional: Filter which models to display
export function filterModels(models: ModelConfig[]): ModelConfig[] {
  return models.filter(m => m.id.includes('chat'));
}
```

## SDK Usage

```typescript
import {
  convertToOpenAI,
  convertToolsToOpenAI,
  processOpenAIStream,
  fetchModelsFromAPI,
  estimateTokens,
} from '@all-in-copilot/sdk';
```

// Dynamic model fetching
const providerConfig: ProviderConfig = {
  id: 'my-provider',
  name: 'My Provider',
  baseUrl: 'https://api.example.com/v1',
  // ... other config
};

const models = await fetchModels(providerConfig, { apiKey: 'your-api-key' });
console.log('Available models:', models);

// Use OpenAI-compatible provider
const provider = new OpenAIProvider(providerConfig, { apiKey: 'your-api-key' });
for (const model of models) {
  provider.registerModel(model);
}

const response = await provider.complete({
  model: models[0].id,
  messages: [{ role: 'user', content: 'Hello!' }],
});
```

## Development

### Build SDK

```bash
cd packages/sdk
npm run build
```

### Build VSCode Extension

```bash
cd packages/vscode
npm run compile
```

### Watch Mode

```bash
# Terminal 1
cd packages/sdk && npm run watch

# Terminal 2
cd packages/vscode && npm run watch
```

## Templates

### MiniMax Template
- **Base URL**: `https://api.minimax.chat/v1/text/chatcompletion_v2`
- **Dynamic Models**: âœ… Enabled
- **Fallback Models**: abab6.5s-chat, abab6.5-chat, abab5.5-chat

### GLM Template (æ™ºè°±AI)
- **Base URL**: `https://open.bigmodel.cn/api/paas/v4/chat/completions`
- **Dynamic Models**: âœ… Enabled
- **Fallback Models**: GLM-4 Plus, GLM-4, GLM-4V, GLM-3 Turbo

### Base Template
- **Purpose**: Blank template for custom providers
- **Dynamic Models**: âœ… Enabled (set `dynamicModels: true` in config)
- **Edit**: `src/config.ts` to configure your provider

## How Dynamic Model Fetching Works

1. Extension calls `/models` endpoint on provider's API
2. Response is parsed and converted to `ModelConfig[]`
3. Models are cached for `modelsCacheTTL` milliseconds
4. If fetch fails, fallback to `FALLBACK_MODELS`
5. Optional `filterModels()` function filters displayed models

```
Provider API (/models)
       â†“
   fetch + cache
       â†“
 filterModels()
       â†“
VS Code Model List
```

## License

MIT
