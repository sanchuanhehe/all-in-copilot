# All-In Copilot

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

ğŸš€ SDK & CLI for building VS Code Chat extensions with custom LLM providers.

### âœ¨ Features

- **One-click CLI Generation** - Interactively create extension projects
- **Multi-provider Presets** - Built-in configurations for GLM, DeepSeek, Qwen, MiniMax, OpenAI, Anthropic
- **Dynamic Model Fetching** - Automatically fetch available models from API
- **Ready-to-use Templates** - Copy templates to quickly create custom Copilot extensions
- **Lightweight SDK** - Core SDK has no VS Code dependencies, works in any Node.js environment

### Architecture

```markdown
all-in-copilot/
â”œâ”€â”€ packages/
â”‚ â””â”€â”€ sdk/ # Core SDK
â”‚ â””â”€â”€ src/
â”‚ â”œâ”€â”€ core/ # Types, model fetcher
â”‚ â””â”€â”€ vscode/ # VS Code provider helpers
â”‚
â”œâ”€â”€ templates/ # Extension templates
â”‚ â”œâ”€â”€ base-template/ # Base template for custom providers
â”‚ â”œâ”€â”€ glm-template/ # GLM (æ™ºè°±AI) example
â”‚ â”œâ”€â”€ minimax-template/ # MiniMax example
â”‚ â”œâ”€â”€ kimi-template/ # Kimi K2 (Moonshot) example
â”‚ â”œâ”€â”€ mimo-template/ # Xiaomi MiMo example
â”‚ â””â”€â”€ aliyun-coding-template/ # Aliyun Model Studio Coding Plan example
â”‚
â””â”€â”€ cli/ # Project generator CLI
â””â”€â”€ src/
â””â”€â”€ index.ts
```

### Quick Start

#### Method 1: Use CLI (Recommended)

```bash
# Install CLI globally
npm install -g @all-in-copilot/cli

# Create a new project interactively
all-in-copilot

# Or with short command
aic create my-copilot
```

#### Method 2: Copy Template

```bash
# Copy template
cp -r templates/glm-template my-copilot
cd my-copilot

# Edit configuration
vim src/config.ts

# Install dependencies
npm install

# Compile and test (F5 in VS Code)
npm run compile
```

### CLI Commands

```bash
all-in-copilot              # Interactive mode
all-in-copilot create NAME  # Create project with prompts
all-in-copilot list         # List available presets
all-in-copilot help         # Show help
```

### Available Presets

| Preset    | Provider        | API Format |
| --------- | --------------- | ---------- |
| glm       | GLM (æ™ºè°±AI)    | OpenAI     |
| minimax   | MiniMax         | Anthropic  |
| kimi      | Kimi (Moonshot) | Anthropic  |
| mimo      | Xiaomi MiMo     | Anthropic  |
| deepseek  | DeepSeek        | OpenAI     |
| qwen      | Qwen            | OpenAI     |
| openai    | OpenAI          | OpenAI     |
| anthropic | Anthropic       | Anthropic  |
| custom    | Custom          | OpenAI     |

### Provider Configuration

Edit `src/config.ts` to customize your provider:

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
	id: "provider-id",
	name: "Provider Name",
	baseUrl: "https://api.example.com/v1/chat/completions",
	apiKeySecret: "extension-name.apiKey",
	family: "provider-family",
	apiMode: "openai", // 'openai' | 'anthropic' | 'gemini' | 'ollama'
	supportsTools: true,
	supportsVision: false,
	defaultMaxOutputTokens: 4096,
	defaultContextLength: 32768,
	dynamicModels: true,
	modelsCacheTTL: 5 * 60 * 1000,
};

export const FALLBACK_MODELS: ModelConfig[] = [
	{
		id: "model-1",
		name: "Model 1",
		maxInputTokens: 30000,
		maxOutputTokens: 4096,
		supportsTools: true,
		supportsVision: false,
	},
];

// Optional: Filter which models to display
export function filterModels(models: ModelConfig[]): ModelConfig[] {
	return models.filter((m) => m.id.includes("chat"));
}
```

### SDK Usage

```typescript
import {
	convertToOpenAI,
	convertToolsToOpenAI,
	processOpenAIStream,
	fetchModelsFromAPI,
	estimateTokens,
} from "@all-in-copilot/sdk";

// Dynamic model fetching
const providerConfig: ProviderConfig = {
	id: "my-provider",
	name: "My Provider",
	baseUrl: "https://api.example.com/v1",
	// ... other config
};

const models = await fetchModels(providerConfig, { apiKey: "your-api-key" });
console.log("Available models:", models);

// Use OpenAI-compatible provider
const provider = new OpenAIProvider(providerConfig, { apiKey: "your-api-key" });
for (const model of models) {
	provider.registerModel(model);
}

const response = await provider.complete({
	model: models[0].id,
	messages: [{ role: "user", content: "Hello!" }],
});
```

### Development

#### Build SDK

```bash
cd packages/sdk
npm run build
```

#### Build VS Code Extension

```bash
cd templates/minimax-template
npm run compile
```

#### Watch Mode

```bash
# Terminal 1
cd packages/sdk && npm run watch

# Terminal 2
cd templates/minimax-template && npm run watch
```

### CI/CD & Publishing

#### GitHub Secrets Required

Add the following secrets in your repository settings:

| Secret     | Description                               |
| ---------- | ----------------------------------------- |
| `VSCE_PAT` | VS Code Marketplace Personal Access Token |
| `OVSX_PAT` | Open VSX Registry Token (optional)        |

#### Workflows

| Workflow           | Trigger                          | Description                                  |
| ------------------ | -------------------------------- | -------------------------------------------- |
| **CI**             | Push/PR to `main`                | Build SDK, CLI and all templates, run tests  |
| **Release**        | Tag push (`v*`) / Manual         | Publish stable or pre-release to marketplace |
| **Pre-release**    | Push to `pre-release/*` / Manual | Publish pre-release version                  |
| **Publish Single** | Manual only                      | Manually publish a single extension          |

#### Release Process

**Stable Release:**

```bash
# 1. Update version
cd templates/glm-template
npm version patch  # or minor / major

# 2. Commit and create tag
git add .
git commit -m "chore: bump glm-template to v0.2.0"
git tag glm-template-v0.2.0
git push origin main --tags
```

**Pre-release:**

```bash
# Method 1: Use tag
git tag glm-template-v0.2.0-beta.1
git push origin --tags

# Method 2: Use branch
git checkout -b pre-release/glm-template
git push origin pre-release/glm-template

# Method 3: Manual trigger via GitHub Actions
```

#### Tag Naming Convention

| Pattern                                  | Triggers                  | Example               |
| ---------------------------------------- | ------------------------- | --------------------- |
| `v*`                                     | Release all templates     | `v1.0.0`              |
| `<template>-v*`                          | Release specific template | `glm-template-v0.2.0` |
| `*-pre*`, `*-alpha*`, `*-beta*`, `*-rc*` | Pre-release               | `v1.0.0-beta.1`       |

### Templates

| Template    | Base URL                                                | API Mode         | Dynamic Models |
| ----------- | ------------------------------------------------------- | ---------------- | -------------- |
| **GLM**     | `https://open.bigmodel.cn/api/paas/v4/chat/completions` | OpenAI           | âœ…             |
| **MiniMax** | `https://api.minimaxi.com/anthropic/v1/messages`        | Anthropic        | âŒ             |
| **Kimi**    | `https://api.moonshot.cn/anthropic`                     | Anthropic        | âŒ             |
| **MiMo**    | `https://api.xiaomimimo.com/anthropic/v1/messages`      | Anthropic        | âŒ             |
| **Aliyun Coding Plan** | `https://coding.dashscope.aliyuncs.com/apps/anthropic/v1/messages` | Anthropic | âŒ |
| **Base**    | Custom                                                  | OpenAI/Anthropic | âœ…             |

### How Dynamic Model Fetching Works

```text
Provider API (/models)
       â†“
   fetch + cache
       â†“
 filterModels()
       â†“
VS Code Model List
```

1. Extension calls `/models` endpoint on provider's API
2. Response is parsed and converted to `ModelConfig[]`
3. Models are cached for `modelsCacheTTL` milliseconds
4. If fetch fails, fallback to `FALLBACK_MODELS`
5. Optional `filterModels()` function filters displayed models

---

## ä¸­æ–‡

ğŸš€ ç”¨äºæ„å»ºå¸¦æœ‰è‡ªå®šä¹‰ LLM æä¾›å•†çš„ VS Code Chat æ‰©å±•çš„ SDK å’Œ CLIã€‚

### âœ¨ åŠŸèƒ½ç‰¹æ€§

- **CLI ä¸€é”®ç”Ÿæˆ** - äº¤äº’å¼åˆ›å»ºæ‰©å±•é¡¹ç›®
- **å¤šä¾›åº”å•†é¢„è®¾** - å†…ç½® GLMã€DeepSeekã€Qwenã€MiniMaxã€OpenAIã€Anthropic é…ç½®
- **åŠ¨æ€æ¨¡å‹è·å–** - è‡ªåŠ¨ä» API è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- **å³ç”¨å‹æ¨¡æ¿** - å¤åˆ¶æ¨¡æ¿å³å¯å¿«é€Ÿåˆ›å»ºè‡ªå®šä¹‰ Copilot æ‰©å±•
- **è½»é‡ SDK** - æ ¸å¿ƒ SDK æ—  VS Code ä¾èµ–ï¼Œå¯åœ¨ä»»ä½• Node.js ç¯å¢ƒä½¿ç”¨

### æ¶æ„

```markdown
all-in-copilot/
â”œâ”€â”€ packages/
â”‚ â””â”€â”€ sdk/ # æ ¸å¿ƒ SDK
â”‚ â””â”€â”€ src/
â”‚ â”œâ”€â”€ core/ # ç±»å‹å®šä¹‰ã€æ¨¡å‹è·å–
â”‚ â””â”€â”€ vscode/ # VS Code æä¾›è€…åŠ©æ‰‹
â”‚
â”œâ”€â”€ templates/ # æ‰©å±•æ¨¡æ¿
â”‚ â”œâ”€â”€ base-template/ # è‡ªå®šä¹‰æä¾›è€…çš„åŸºç¡€æ¨¡æ¿
â”‚ â”œâ”€â”€ glm-template/ # GLM (æ™ºè°±AI) ç¤ºä¾‹
â”‚ â”œâ”€â”€ minimax-template/ # MiniMax ç¤ºä¾‹
â”‚ â”œâ”€â”€ kimi-template/ # Kimi K2 (æœˆä¹‹æš—é¢) ç¤ºä¾‹
â”‚ â”œâ”€â”€ mimo-template/ # å°ç±³ MiMo ç¤ºä¾‹
â”‚ â””â”€â”€ aliyun-coding-template/ # é˜¿é‡Œäº‘ç™¾ç‚¼ç¼–ç¨‹è®¡åˆ’ç¤ºä¾‹
â”‚
â””â”€â”€ cli/ # é¡¹ç›®ç”Ÿæˆ CLI
â””â”€â”€ src/
â””â”€â”€ index.ts
```

### å¿«é€Ÿå¼€å§‹

#### æ–¹æ³• 1ï¼šä½¿ç”¨ CLIï¼ˆæ¨èï¼‰

```bash
# å…¨å±€å®‰è£… CLI
npm install -g @all-in-copilot/cli

# äº¤äº’å¼åˆ›å»ºæ–°é¡¹ç›®
all-in-copilot

# æˆ–ä½¿ç”¨ç®€çŸ­å‘½ä»¤
aic create my-copilot
```

#### æ–¹æ³• 2ï¼šå¤åˆ¶æ¨¡æ¿

```bash
# å¤åˆ¶æ¨¡æ¿
cp -r templates/glm-template my-copilot
cd my-copilot

# ç¼–è¾‘é…ç½®
vim src/config.ts

# å®‰è£…ä¾èµ–
npm install

# ç¼–è¯‘å¹¶æµ‹è¯•ï¼ˆåœ¨ VS Code ä¸­æŒ‰ F5ï¼‰
npm run compile
```

### CLI å‘½ä»¤

```bash
all-in-copilot              # äº¤äº’æ¨¡å¼
all-in-copilot create NAME  # ä½¿ç”¨æç¤ºåˆ›å»ºé¡¹ç›®
all-in-copilot list         # åˆ—å‡ºå¯ç”¨é¢„è®¾
all-in-copilot help         # æ˜¾ç¤ºå¸®åŠ©
```

### å¯ç”¨é¢„è®¾

| é¢„è®¾      | æä¾›å•†          | API æ ¼å¼  |
| --------- | --------------- | --------- |
| glm       | GLM (æ™ºè°±AI)    | OpenAI    |
| minimax   | MiniMax         | Anthropic |
| kimi      | Kimi (æœˆä¹‹æš—é¢) | Anthropic |
| mimo      | å°ç±³ MiMo       | Anthropic |
| deepseek  | DeepSeek        | OpenAI    |
| qwen      | Qwen (é€šä¹‰åƒé—®) | OpenAI    |
| openai    | OpenAI          | OpenAI    |
| anthropic | Anthropic       | Anthropic |
| custom    | è‡ªå®šä¹‰          | OpenAI    |

### æä¾›è€…é…ç½®

ç¼–è¾‘ `src/config.ts` æ¥è‡ªå®šä¹‰ä½ çš„æä¾›è€…ï¼š

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
	id: "provider-id", // æä¾›è€… ID
	name: "Provider Name", // æ˜¾ç¤ºåç§°
	baseUrl: "https://api.example.com/v1/chat/completions", // API åœ°å€
	apiKeySecret: "extension-name.apiKey", // API å¯†é’¥å­˜å‚¨é”®
	family: "provider-family", // æ¨¡å‹ç³»åˆ—
	apiMode: "openai", // API æ¨¡å¼ï¼š'openai' | 'anthropic' | 'gemini' | 'ollama'
	supportsTools: true, // æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
	supportsVision: false, // æ˜¯å¦æ”¯æŒå›¾åƒ
	defaultMaxOutputTokens: 4096,
	defaultContextLength: 32768,
	dynamicModels: true, // æ˜¯å¦åŠ¨æ€è·å–æ¨¡å‹
	modelsCacheTTL: 5 * 60 * 1000, // æ¨¡å‹ç¼“å­˜æ—¶é—´
};

export const FALLBACK_MODELS: ModelConfig[] = [
	{
		id: "model-1",
		name: "Model 1",
		maxInputTokens: 30000,
		maxOutputTokens: 4096,
		supportsTools: true,
		supportsVision: false,
	},
];

// å¯é€‰ï¼šè¿‡æ»¤è¦æ˜¾ç¤ºçš„æ¨¡å‹
export function filterModels(models: ModelConfig[]): ModelConfig[] {
	return models.filter((m) => m.id.includes("chat"));
}
```

### SDK ä½¿ç”¨

```typescript
import {
	convertToOpenAI,
	convertToolsToOpenAI,
	processOpenAIStream,
	fetchModelsFromAPI,
	estimateTokens,
} from "@all-in-copilot/sdk";

// åŠ¨æ€æ¨¡å‹è·å–
const providerConfig: ProviderConfig = {
	id: "my-provider",
	name: "My Provider",
	baseUrl: "https://api.example.com/v1",
	// ... å…¶ä»–é…ç½®
};

const models = await fetchModels(providerConfig, { apiKey: "your-api-key" });
console.log("å¯ç”¨æ¨¡å‹:", models);

// ä½¿ç”¨ OpenAI å…¼å®¹çš„æä¾›è€…
const provider = new OpenAIProvider(providerConfig, { apiKey: "your-api-key" });
for (const model of models) {
	provider.registerModel(model);
}

const response = await provider.complete({
	model: models[0].id,
	messages: [{ role: "user", content: "Hello!" }],
});
```

### å¼€å‘

#### æ„å»º SDK

```bash
cd packages/sdk
npm run build
```

#### æ„å»º VS Code æ‰©å±•

```bash
cd templates/minimax-template
npm run compile
```

#### ç›‘è§†æ¨¡å¼

```bash
# ç»ˆç«¯ 1
cd packages/sdk && npm run watch

# ç»ˆç«¯ 2
cd templates/minimax-template && npm run watch
```

### CI/CD å’Œå‘å¸ƒ

#### éœ€è¦é…ç½®çš„ GitHub Secrets

åœ¨ä»“åº“è®¾ç½®ä¸­æ·»åŠ ä»¥ä¸‹ Secretsï¼š

| Secret     | æè¿°                         |
| ---------- | ---------------------------- |
| `VSCE_PAT` | VS Code æ’ä»¶å¸‚åœºä¸ªäººè®¿é—®ä»¤ç‰Œ |
| `OVSX_PAT` | Open VSX æ³¨å†Œè¡¨ä»¤ç‰Œï¼ˆå¯é€‰ï¼‰  |

#### å·¥ä½œæµ

| å·¥ä½œæµ             | è§¦å‘æ¡ä»¶                       | æè¿°                               |
| ------------------ | ------------------------------ | ---------------------------------- |
| **CI**             | Push/PR åˆ° `main`              | æ„å»º SDKã€CLI å’Œæ‰€æœ‰æ¨¡æ¿ï¼Œè¿è¡Œæµ‹è¯• |
| **Release**        | Tag æ¨é€ (`v*`) / æ‰‹åŠ¨         | å‘å¸ƒæ­£å¼ç‰ˆæˆ–é¢„å‘å¸ƒç‰ˆåˆ°æ’ä»¶å¸‚åœº     |
| **Pre-release**    | Push åˆ° `pre-release/*` / æ‰‹åŠ¨ | å‘å¸ƒé¢„å‘å¸ƒç‰ˆæœ¬                     |
| **Publish Single** | ä»…æ‰‹åŠ¨                         | æ‰‹åŠ¨å‘å¸ƒå•ä¸ªæ’ä»¶                   |

#### å‘å¸ƒæµç¨‹

**æ­£å¼å‘å¸ƒï¼š**

```bash
# 1. æ›´æ–°ç‰ˆæœ¬å·
cd templates/glm-template
npm version patch  # æˆ– minor / major

# 2. æäº¤å¹¶åˆ›å»º tag
git add .
git commit -m "chore: bump glm-template to v0.2.0"
git tag glm-template-v0.2.0
git push origin main --tags
```

**é¢„å‘å¸ƒï¼š**

```bash
# æ–¹æ³• 1ï¼šä½¿ç”¨ Tag
git tag glm-template-v0.2.0-beta.1
git push origin --tags

# æ–¹æ³• 2ï¼šä½¿ç”¨åˆ†æ”¯
git checkout -b pre-release/glm-template
git push origin pre-release/glm-template

# æ–¹æ³• 3ï¼šé€šè¿‡ GitHub Actions æ‰‹åŠ¨è§¦å‘
```

#### Tag å‘½åè§„èŒƒ

| æ¨¡å¼                                     | è§¦å‘         | ç¤ºä¾‹                  |
| ---------------------------------------- | ------------ | --------------------- |
| `v*`                                     | å‘å¸ƒæ‰€æœ‰æ¨¡æ¿ | `v1.0.0`              |
| `<template>-v*`                          | å‘å¸ƒæŒ‡å®šæ¨¡æ¿ | `glm-template-v0.2.0` |
| `*-pre*`, `*-alpha*`, `*-beta*`, `*-rc*` | é¢„å‘å¸ƒ       | `v1.0.0-beta.1`       |

### æ¨¡æ¿

| æ¨¡æ¿        | Base URL                                                | API æ¨¡å¼         | åŠ¨æ€æ¨¡å‹ |
| ----------- | ------------------------------------------------------- | ---------------- | -------- |
| **GLM**     | `https://open.bigmodel.cn/api/paas/v4/chat/completions` | OpenAI           | âœ…       |
| **MiniMax** | `https://api.minimaxi.com/anthropic/v1/messages`        | Anthropic        | âŒ       |
| **Kimi**    | `https://api.moonshot.cn/anthropic`                     | Anthropic        | âŒ       |
| **MiMo**    | `https://api.xiaomimimo.com/anthropic/v1/messages`      | Anthropic        | âŒ       |
| **Aliyun Coding Plan** | `https://coding.dashscope.aliyuncs.com/apps/anthropic/v1/messages` | Anthropic | âŒ |
| **Base**    | è‡ªå®šä¹‰                                                  | OpenAI/Anthropic | âœ…       |

### åŠ¨æ€æ¨¡å‹è·å–å·¥ä½œåŸç†

```text
æä¾›è€… API (/models)
       â†“
   è·å– + ç¼“å­˜
       â†“
 filterModels()
       â†“
VS Code æ¨¡å‹åˆ—è¡¨
```

1. æ‰©å±•è°ƒç”¨æä¾›è€… API çš„ `/models` ç«¯ç‚¹
2. å“åº”è¢«è§£æå¹¶è½¬æ¢ä¸º `ModelConfig[]`
3. æ¨¡å‹è¢«ç¼“å­˜ `modelsCacheTTL` æ¯«ç§’
4. å¦‚æœè·å–å¤±è´¥ï¼Œå›é€€åˆ° `FALLBACK_MODELS`
5. å¯é€‰çš„ `filterModels()` å‡½æ•°è¿‡æ»¤è¦æ˜¾ç¤ºçš„æ¨¡å‹

---

## Acknowledgments / è‡´è°¢

This project is inspired by and references the following excellent open source projects:

æœ¬é¡¹ç›®å—åˆ°ä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®çš„å¯å‘å’Œå‚è€ƒï¼š

### Core References / æ ¸å¿ƒå‚è€ƒ

| Project                                                                 | Description                                                                                                                                                                       |
| ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [vscode-copilot-chat](https://github.com/microsoft/vscode-copilot-chat) | Microsoft's official GitHub Copilot Chat extension - the authoritative reference for VS Code Chat API usage / å¾®è½¯å®˜æ–¹ GitHub Copilot Chat æ‰©å±• - VS Code Chat API ä½¿ç”¨çš„æƒå¨å‚è€ƒ |
| [vscode](https://github.com/microsoft/vscode)                           | Visual Studio Code source code - for understanding VS Code extension APIs / VS Code æºä»£ç  - ç”¨äºç†è§£ VS Code æ‰©å±• API                                                            |

### Community Projects / ç¤¾åŒºé¡¹ç›®

| Project                                                                           | Description                                                                                                                |
| --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| [oai-compatible-copilot](https://github.com/AzurCodin/oai-compatible-copilot)     | OpenAI-compatible Copilot - early exploration of custom model integration / OpenAI å…¼å®¹ Copilot - è‡ªå®šä¹‰æ¨¡å‹é›†æˆçš„æ—©æœŸæ¢ç´¢ |
| [huggingface-vscode-chat](https://github.com/huggingface/huggingface-vscode-chat) | HuggingFace's VS Code Chat extension / HuggingFace çš„ VS Code Chat æ‰©å±•                                                    |
| [addi](https://github.com/deepwn/addi)                                            | AI-powered development assistant with MCP integration / å¸¦ MCP é›†æˆçš„ AI å¼€å‘åŠ©æ‰‹                                          |
| [ChatGLM-vscode-chat](https://github.com/AzurCodin/ChatGLM-vscode-chat)           | ChatGLM VS Code extension - GLM model integration reference / ChatGLM VS Code æ‰©å±• - GLM æ¨¡å‹é›†æˆå‚è€ƒ                      |

### AI Providers / AI æœåŠ¡æä¾›å•†

Special thanks to the following AI providers for their excellent APIs:

ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹ AI æœåŠ¡æä¾›å•†æä¾›çš„ä¼˜ç§€ APIï¼š

- [æ™ºè°±AI (Zhipu AI)](https://open.bigmodel.cn/) - GLM series models / GLM ç³»åˆ—æ¨¡å‹
- [Moonshot AI (æœˆä¹‹æš—é¢)](https://platform.moonshot.cn/) - Kimi K2 models / Kimi K2 æ¨¡å‹
- [MiniMax](https://www.minimax.chat/) - M2 series models / M2 ç³»åˆ—æ¨¡å‹
- [Xiaomi MiMo (å°ç±³)](https://xiaomimimo.com/) - MiMo models / MiMo æ¨¡å‹
- [OpenAI](https://openai.com/) - GPT models / GPT æ¨¡å‹
- [Anthropic](https://anthropic.com/) - Claude models / Claude æ¨¡å‹
- [Google](https://ai.google.dev/) - Gemini models / Gemini æ¨¡å‹

### Documentation / æ–‡æ¡£å‚è€ƒ

- [VS Code Extension API](https://code.visualstudio.com/api)
- [VS Code Chat Extension Guide](https://code.visualstudio.com/api/extension-guides/chat)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic API Reference](https://docs.anthropic.com/en/api)

## Contributing / è´¡çŒ®

Contributions are welcome! Please feel free to submit a Pull Request.

æ¬¢è¿è´¡çŒ®ï¼è¯·éšæ—¶æäº¤ Pull Requestã€‚

## License / è®¸å¯è¯

MIT
