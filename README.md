# All-In Copilot

üöÄ SDK & CLI for building VS Code Chat extensions with custom LLM providers.

## ‚ú® Features

- **CLI ‰∏ÄÈîÆÁîüÊàê**: ‰∫§‰∫íÂºèÂàõÂª∫Êâ©Â±ïÈ°πÁõÆ
- **Â§ö‰æõÂ∫îÂïÜÈ¢ÑËÆæ**: ÂÜÖÁΩÆ GLM„ÄÅDeepSeek„ÄÅQwen„ÄÅMiniMax„ÄÅOpenAI„ÄÅAnthropic ÈÖçÁΩÆ
- **Âä®ÊÄÅÊ®°ÂûãËé∑Âèñ**: Ëá™Âä®‰ªé API Ëé∑ÂèñÂèØÁî®Ê®°ÂûãÂàóË°®
- **Âç≥Áî®ÂûãÊ®°Êùø**: Â§çÂà∂Ê®°ÊùøÂç≥ÂèØÂø´ÈÄüÂàõÂª∫Ëá™ÂÆö‰πâ Copilot Êâ©Â±ï
- **ËΩªÈáè SDK**: Ê†∏ÂøÉ SDK Êó† VS Code ‰æùËµñÔºåÂèØÂú®‰ªª‰Ωï Node.js ÁéØÂ¢É‰ΩøÁî®

## Architecture

```
all-in-copilot/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ sdk/              # Core SDK
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îú‚îÄ‚îÄ core/     # Types, model fetcher
‚îÇ           ‚îî‚îÄ‚îÄ vscode/   # VS Code provider helpers
‚îÇ
‚îú‚îÄ‚îÄ templates/            # Extension templates
‚îÇ   ‚îú‚îÄ‚îÄ base-template/    # Base template for custom providers
‚îÇ   ‚îú‚îÄ‚îÄ glm-template/     # GLM (Êô∫Ë∞±AI) example
‚îÇ   ‚îî‚îÄ‚îÄ minimax-template/ # MiniMax example
‚îÇ
‚îî‚îÄ‚îÄ cli/                  # Project generator CLI
    ‚îî‚îÄ‚îÄ src/
        ‚îî‚îÄ‚îÄ index.ts
```

## Quick Start

### Method 1: Use CLI (Recommended)

```bash
# Install CLI globally
npm install -g @all-in-copilot/cli

# Create a new project interactively
all-in-copilot

# Or with short command
aic create my-copilot
```

### Method 2: Copy Template

```bash
# Copy template
cp -r templates/glm-template my-copilot
cd my-copilot

# Edit configuration
vim src/config.ts

# Install dependencies
npm install

# Compile and test (F5 in VS Code)
npm run compile
```

## CLI Commands

```bash
all-in-copilot              # Interactive mode
all-in-copilot create NAME  # Create project with prompts
all-in-copilot list         # List available presets
all-in-copilot help         # Show help
```

### Available Presets

| Preset    | Provider       | API Format |
|-----------|----------------|------------|
| glm       | GLM (Êô∫Ë∞±AI)   | OpenAI     |
| minimax   | MiniMax        | Anthropic  |
| deepseek  | DeepSeek       | OpenAI     |
| qwen      | Qwen (ÈÄö‰πâÂçÉÈóÆ) | OpenAI     |
| openai    | OpenAI         | OpenAI     |
| anthropic | Anthropic      | Anthropic  |
| custom    | Custom         | OpenAI     |

## Provider Configuration

Edit `src/config.ts` to customize your provider:

```typescript
export const PROVIDER_CONFIG: ProviderConfig = {
  id: 'provider-id',
  name: 'Provider Name',
  baseUrl: 'https://api.example.com/v1/chat/completions',
  apiKeySecret: 'extension-name.apiKey',
  family: 'provider-family',
  apiMode: 'openai',  // 'openai' | 'anthropic' | 'gemini' | 'ollama'
  supportsTools: true,
  supportsVision: false,
  defaultMaxOutputTokens: 4096,
  defaultContextLength: 32768,
  dynamicModels: true,
  modelsCacheTTL: 5 * 60 * 1000,
};

export const FALLBACK_MODELS: ModelConfig[] = [
  {
    id: 'model-1',
    name: 'Model 1',
    maxInputTokens: 30000,
    maxOutputTokens: 4096,
    supportsTools: true,
    supportsVision: false,
  },
];

// Optional: Filter which models to display
export function filterModels(models: ModelConfig[]): ModelConfig[] {
  return models.filter(m => m.id.includes('chat'));
}
```

## SDK Usage

```typescript
import {
  convertToOpenAI,
  convertToolsToOpenAI,
  processOpenAIStream,
  fetchModelsFromAPI,
  estimateTokens,
} from '@all-in-copilot/sdk';
```

// Dynamic model fetching
const providerConfig: ProviderConfig = {
  id: 'my-provider',
  name: 'My Provider',
  baseUrl: 'https://api.example.com/v1',
  // ... other config
};

const models = await fetchModels(providerConfig, { apiKey: 'your-api-key' });
console.log('Available models:', models);

// Use OpenAI-compatible provider
const provider = new OpenAIProvider(providerConfig, { apiKey: 'your-api-key' });
for (const model of models) {
  provider.registerModel(model);
}

const response = await provider.complete({
  model: models[0].id,
  messages: [{ role: 'user', content: 'Hello!' }],
});
```

## Development

### Build SDK

```bash
cd packages/sdk
npm run build
```

### Build VSCode Extension

```bash
cd packages/vscode
npm run compile
```

### Watch Mode

```bash
# Terminal 1
cd packages/sdk && npm run watch

# Terminal 2
cd packages/vscode && npm run watch
```

## Templates

### MiniMax Template
- **Base URL**: `https://api.minimaxi.com/anthropic/v1/messages`
- **API Mode**: Anthropic (‰ΩøÁî® Anthropic ÂÖºÂÆπÊé•Âè£)
- **Dynamic Models**: ‚ùå Disabled (‰ΩøÁî®È¢ÑÂÆö‰πâÊ®°ÂûãÂàóË°®)
- **Fallback Models**: MiniMax-M2.1, MiniMax-M2.1-lightning, MiniMax-M2

### GLM Template (Êô∫Ë∞±AI)
- **Base URL**: `https://open.bigmodel.cn/api/paas/v4/chat/completions`
- **API Mode**: OpenAI
- **Dynamic Models**: ‚úÖ Enabled
- **Fallback Models**: GLM-4 Plus, GLM-4, GLM-4V, GLM-3 Turbo

### Base Template
- **Purpose**: Blank template for custom providers
- **API Mode**: Supports both OpenAI and Anthropic formats
- **Dynamic Models**: ‚úÖ Enabled (set `dynamicModels: true` in config)
- **Edit**: `src/config.ts` to configure your provider

## How Dynamic Model Fetching Works

1. Extension calls `/models` endpoint on provider's API
2. Response is parsed and converted to `ModelConfig[]`
3. Models are cached for `modelsCacheTTL` milliseconds
4. If fetch fails, fallback to `FALLBACK_MODELS`
5. Optional `filterModels()` function filters displayed models

```
Provider API (/models)
       ‚Üì
   fetch + cache
       ‚Üì
 filterModels()
       ‚Üì
VS Code Model List
```

## License

MIT
